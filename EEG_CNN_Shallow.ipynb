{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io as sciio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = np.array([[[1,1,1,1],[2,2,2,2],[3,3,3,3]],[[4,4,4,4],[5,5,5,5],[6,6,6,6]]])\n",
    "print('X=',X,X.shape)\n",
    "Y = tf.transpose(X,[2,1,0])\n",
    "with tf.Session() as sess:\n",
    "    print('Y =',sess.run(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_MAT(file_dir):\n",
    "    target = sciio.loadmat(file_dir)\n",
    "    pre_raw_data = list(target.values())\n",
    "    raw_data = np.array(pre_raw_data)\n",
    "    real_data = raw_data[3:107]   \n",
    "\n",
    "    Row = real_data.reshape(( -1 , 1))\n",
    "\n",
    "    steps = 100\n",
    "    i = -1\n",
    "    A_Z = np.empty((100,64,700))\n",
    "    for step in range(steps):\n",
    "        i = i+1\n",
    "        A_Z[i,] = Row[i][0][0:64]\n",
    "#print(A_F)\n",
    "    AZ = A_Z.reshape(100,64,700,1)\n",
    "#print(AF.shape)\n",
    "    return AZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSS = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_A = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/A')\n",
    "BATCH_B = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/B')\n",
    "BATCH_C = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/C')\n",
    "BATCH_D = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/D')\n",
    "BATCH_E = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/E')\n",
    "BATCH_F = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/F')\n",
    "BATCH_G = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/G')\n",
    "BATCH_H = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/H')\n",
    "BATCH_I = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/I')\n",
    "BATCH_J = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/J')\n",
    "BATCH_K = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/K')\n",
    "BATCH_L = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/L')\n",
    "BATCH_M = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/M')\n",
    "BATCH_N = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/N')\n",
    "BATCH_O = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/O')\n",
    "BATCH_P = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/P')\n",
    "BATCH_Q = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/Q')\n",
    "BATCH_R = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/R')\n",
    "BATCH_S = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/S')\n",
    "BATCH_T = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/T')\n",
    "BATCH_U = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/U')\n",
    "BATCH_V = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/V')\n",
    "BATCH_W = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/W')\n",
    "BATCH_X = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/CT data-mat/X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_batch_label_0 = np.array([BATCH_A,BATCH_B,BATCH_C,BATCH_D,BATCH_E,BATCH_F,BATCH_G,\n",
    "                                BATCH_H,BATCH_I,BATCH_J,BATCH_K,BATCH_L,BATCH_M,BATCH_N,\n",
    "                                BATCH_O,BATCH_P,BATCH_Q,BATCH_R,BATCH_S,BATCH_T,BATCH_U])\n",
    "Test_batch_label_0 = np.array([BATCH_V,BATCH_W,BATCH_X])#TRAIN有21个，test有3个，7:1\n",
    "#print(batch_label_0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_1 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/1')\n",
    "BATCH_2 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/2')\n",
    "BATCH_3 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/3')\n",
    "BATCH_4 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/4')\n",
    "BATCH_5 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/5')\n",
    "BATCH_6 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/6')\n",
    "BATCH_7 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/7')\n",
    "BATCH_8 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/8')\n",
    "BATCH_9 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/9')\n",
    "BATCH_10 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/10')\n",
    "BATCH_11 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/11')\n",
    "BATCH_12 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/12')\n",
    "BATCH_13 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/13')\n",
    "BATCH_14 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/14')\n",
    "BATCH_15 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/15')\n",
    "#BATCH_16 = get_data_from_MAT(file_dir='/mnt/shareEx/shixinjie/PD_data-mat/16')\n",
    "BATCH_17 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/17')\n",
    "BATCH_18 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/18')\n",
    "BATCH_19 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/19')\n",
    "BATCH_20 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/20')\n",
    "BATCH_21 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/21')\n",
    "BATCH_22 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/22')\n",
    "BATCH_23 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/23')\n",
    "BATCH_24 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/24')\n",
    "BATCH_25 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/25')\n",
    "BATCH_26 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/26')\n",
    "BATCH_27 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/27')\n",
    "BATCH_28 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/28')\n",
    "BATCH_29 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/29')\n",
    "BATCH_30 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/30')\n",
    "BATCH_31 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/31')\n",
    "BATCH_32 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/32')\n",
    "BATCH_33 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/33')\n",
    "BATCH_34 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/34')\n",
    "BATCH_35 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/35')\n",
    "BATCH_36 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/36')\n",
    "BATCH_37 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/37')\n",
    "BATCH_38 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/38')\n",
    "BATCH_39 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/39')\n",
    "BATCH_40 = get_data_from_MAT(file_dir='C:/Users/sq/Desktop/EEG-DATA/PD_data-mat/40')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55, 100, 64, 700, 1) (8, 100, 64, 700, 1)\n"
     ]
    }
   ],
   "source": [
    "Train_batch_label_1 = np.array([BATCH_1,BATCH_2,BATCH_3,BATCH_4,BATCH_5,BATCH_6,BATCH_7,\n",
    "                          BATCH_8,BATCH_9,BATCH_10,BATCH_11,BATCH_12,BATCH_13,BATCH_14,\n",
    "                          BATCH_15,         BATCH_17,BATCH_18,BATCH_19,BATCH_20,BATCH_21,\n",
    "                          BATCH_22,BATCH_23,BATCH_24,BATCH_25,BATCH_26,BATCH_27,BATCH_28,\n",
    "                          BATCH_29,BATCH_30,BATCH_31,BATCH_32,BATCH_33,BATCH_34,BATCH_35])\n",
    "Test_batch_label_1 = np.array([BATCH_36,BATCH_37,BATCH_38,BATCH_39,BATCH_40])#训练的\n",
    "#print(batch_label_1[0].shape,batch_label_1[0])\n",
    "Train_batch_f0l1 = np.array([BATCH_A,BATCH_B,BATCH_C,BATCH_D,BATCH_E,BATCH_F,BATCH_G,\n",
    "                             BATCH_H,BATCH_I,BATCH_J,BATCH_K,BATCH_L,BATCH_M,BATCH_N,\n",
    "                             BATCH_O,BATCH_P,BATCH_Q,BATCH_R,BATCH_S,BATCH_T,BATCH_U,\n",
    "                        BATCH_1,BATCH_2,BATCH_3,BATCH_4,BATCH_5,BATCH_6,BATCH_7,\n",
    "                        BATCH_8,BATCH_9,BATCH_10,BATCH_11,BATCH_12,BATCH_13,BATCH_14,\n",
    "                        BATCH_15,         BATCH_17,BATCH_18,BATCH_19,BATCH_20,BATCH_21,\n",
    "                        BATCH_22,BATCH_23,BATCH_24,BATCH_25,BATCH_26,BATCH_27,BATCH_28,\n",
    "                        BATCH_29,BATCH_30,BATCH_31,BATCH_32,BATCH_33,BATCH_34,BATCH_35])  #[0:21]个label为0,,[21:55]是label为1\n",
    "  \n",
    "Test_batch_f0l1 = np.array([BATCH_V,BATCH_W,BATCH_X,\n",
    "                           BATCH_36,BATCH_37,BATCH_38,BATCH_39,BATCH_40])      #[0:3]是label为0，[3:8]是label为1\n",
    "\n",
    "print(Train_batch_f0l1.shape,Test_batch_f0l1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_with_weight_loss(shape, stddev, w1):\n",
    "    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "    if w1 is not None:\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(var),w1,name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_loss)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100 \n",
    "\n",
    "TrainEEG_holder = tf.placeholder(tf.float32, [batch_size, 64, 700, 1])#训练的样本为高乘宽64*700，通道为1，[batch, height, width, channels]\n",
    "TrainLabel_holder = tf.placeholder(tf.int32, [batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 675, 40) (100, 40, 675) (100, 40, 675, 1)\n"
     ]
    }
   ],
   "source": [
    "weight1 = variable_with_weight_loss(shape=[1,26,1,40], stddev=5e-2, w1=0.0)                   \n",
    "kernel1 = tf.nn.conv2d(TrainEEG_holder, weight1, [1,1,1,1], padding='VALID')\n",
    "bias1 = tf.Variable(tf.constant(0.0, shape=[40]))\n",
    "conv1 = tf.nn.relu(tf.nn.bias_add(kernel1, bias1))\n",
    "'''-------'''\n",
    "conv1_pre = tf.transpose(conv1,[0,2,1,3])\n",
    "conv1_t = tf.reshape(conv1_pre,[100,675,64,40,1])\n",
    "'''-------'''\n",
    "weight1_1 = variable_with_weight_loss(shape=[1,64,40,1,40], stddev=5e-2, w1=0.0)#三维卷积，卷积核的[batch,hight,width,deepth,out-channel]\n",
    "kernel1_1 = tf.nn.conv3d(conv1_t, weight1_1, [1,1,1,1,1], padding='VALID')\n",
    "bias1_1 = tf.Variable(tf.constant(0.0, shape=[40]))#有40个卷积核,输出为40*\n",
    "conv1_1 = tf.nn.relu(tf.nn.bias_add(kernel1_1,bias1_1))#(100, 675, 1, 1, 40)\n",
    "'''-------'''\n",
    "conv1_1shape = tf.reshape(conv1_1,[100,675,40])\n",
    "conv1_1true = tf.transpose(conv1_1shape,[0,2,1])\n",
    "conv1_for_pool1 = tf.reshape(conv1_1true,[100,40,675,1])\n",
    "print(conv1_1shape.shape,conv1_1true.shape,conv1_for_pool1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 40, 41, 1)\n"
     ]
    }
   ],
   "source": [
    "pool1 = tf.nn.avg_pool(conv1_for_pool1, ksize=[1, 1, 75, 1], strides=[1, 1, 15, 1], padding='VALID')\n",
    "norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "print(norm1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1640)\n",
      "1640\n"
     ]
    }
   ],
   "source": [
    "reshape = tf.reshape(norm1,[batch_size,-1])\n",
    "print(reshape.shape)\n",
    "dim = reshape.get_shape()[1].value\n",
    "print(dim)\n",
    "weight3 = variable_with_weight_loss(shape=[dim,100], stddev=0.04, w1=0.004)#设置隐藏层为100，\n",
    "bias3 = tf.Variable(tf.constant(0.1, shape=[100]))\n",
    "local3 = tf.nn.relu(tf.matmul(reshape, weight3) + bias3)\n",
    "weight4 = variable_with_weight_loss(shape=[100,2], stddev=0.04, w1=0.004)#100个隐含节点变为2\n",
    "bias4 = tf.Variable(tf.constant(0.1, shape=[2]))\n",
    "logits = tf.add(tf.matmul(local3, weight4),bias4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=labels,name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy,name='cross_entropy')\n",
    "    tf.add_to_collection('losses',cross_entropy_mean)\n",
    "    return tf.add_n(tf.get_collection('losses'),name='total_loss')\n",
    "'''--------'''\n",
    "loss = loss(logits, TrainLabel_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "'''--------'''\n",
    "top_k_op = tf.nn.in_top_k(logits, TrainLabel_holder, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-30-fe930051e032>:1: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.start_queue_runners()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Test_batch_label_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中label为0的大小： (21, 100, 64, 700, 1)\n",
      "训练集中label为1的大小： (34, 100, 64, 700, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"训练集中label为0的大小：\",Train_batch_label_0.shape)\n",
    "print(\"训练集中label为1的大小：\",Train_batch_label_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集中label为0的大小： (3, 100, 64, 700, 1)\n",
      "测试集中label为1的大小： (5, 100, 64, 700, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"测试集中label为0的大小：\",Test_batch_label_0.shape)\n",
    "print(\"测试集中label为1的大小：\",Test_batch_label_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "'''一个batch的label'''\n",
    "label_batch_1 = np.ones((100),dtype=int)\n",
    "print(label_batch_1)\n",
    "label_batch_0 = np.zeros((100),dtype=int)\n",
    "print(label_batch_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 55 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step is 0\n",
      "loss is [None, 2.0624728]\n",
      "examples_per_sec is 2.1413885585835346\n",
      "sec/batch： 46.698671102523804\n",
      "step is 3\n",
      "loss is [None, 0.7430075]\n",
      "examples_per_sec is 6.531731660342865\n",
      "sec/batch： 15.309875726699829\n",
      "step is 6\n",
      "loss is [None, 0.67638046]\n",
      "examples_per_sec is 7.96830660034213\n",
      "sec/batch： 12.549717903137207\n",
      "step is 9\n",
      "loss is [None, 0.61515117]\n",
      "examples_per_sec is 12.691240238567131\n",
      "sec/batch： 7.879450559616089\n",
      "step is 12\n",
      "loss is [None, 0.5596592]\n",
      "examples_per_sec is 12.383192649321652\n",
      "sec/batch： 8.075461864471436\n",
      "step is 15\n",
      "loss is [None, 0.5098052]\n",
      "examples_per_sec is 12.877563662947539\n",
      "sec/batch： 7.765444040298462\n",
      "step is 18\n",
      "loss is [None, 0.465303]\n",
      "examples_per_sec is 13.124331353770218\n",
      "sec/batch： 7.619435787200928\n",
      "step is 21\n",
      "loss is [None, 99.82064]\n",
      "examples_per_sec is 13.244275770927391\n",
      "sec/batch： 7.550431728363037\n",
      "step is 24\n",
      "loss is [None, 41.18804]\n",
      "examples_per_sec is 12.689629499961365\n",
      "sec/batch： 7.88045072555542\n",
      "step is 27\n",
      "loss is [None, 8.899849]\n",
      "examples_per_sec is 12.186993271460011\n",
      "sec/batch： 8.205469369888306\n",
      "step is 30\n",
      "loss is [None, 1.1636829]\n",
      "examples_per_sec is 8.520303433264807\n",
      "sec/batch： 11.736671209335327\n",
      "step is 33\n",
      "loss is [None, 0.7048886]\n",
      "examples_per_sec is 9.817798701962927\n",
      "sec/batch： 10.185582637786865\n",
      "step is 36\n",
      "loss is [None, 0.38156268]\n",
      "examples_per_sec is 7.568443890227336\n",
      "sec/batch： 13.212755680084229\n",
      "step is 39\n",
      "loss is [None, 0.31033367]\n",
      "examples_per_sec is 7.559289260596494\n",
      "sec/batch： 13.22875690460205\n",
      "step is 42\n",
      "loss is [None, 0.27800456]\n",
      "examples_per_sec is 11.78760820403034\n",
      "sec/batch： 8.483485221862793\n",
      "step is 45\n",
      "loss is [None, 0.27549]\n",
      "examples_per_sec is 11.374619395504222\n",
      "sec/batch： 8.791502952575684\n",
      "step is 48\n",
      "loss is [None, 0.2575384]\n",
      "examples_per_sec is 10.781054428426403\n",
      "sec/batch： 9.275530576705933\n",
      "step is 51\n",
      "loss is [None, 0.24912025]\n",
      "examples_per_sec is 11.617785712483304\n",
      "sec/batch： 8.607492208480835\n",
      "step is 54\n",
      "loss is [None, 0.2415293]\n",
      "examples_per_sec is 11.533365899760756\n",
      "sec/batch： 8.670495748519897\n"
     ]
    }
   ],
   "source": [
    "#max_steps变为52\n",
    "import time\n",
    "i = -1\n",
    "for step in range(max_steps):\n",
    "    start_time = time.time()\n",
    "    #image_batch,label_batch = sess.run([images_train, labels_train]),   #每一步都会有新的数据feed进来，通过调用CIFAR-10里面的写好的函数。\n",
    "    #_, \n",
    "    i = i+1\n",
    "    if i < 21:\n",
    "        loss_value = sess.run([train_op, loss],\n",
    "                          feed_dict=\n",
    "                          {TrainEEG_holder: Train_batch_f0l1[i], \n",
    "                           TrainLabel_holder:\n",
    "                           label_batch_0\n",
    "                          }\n",
    "                         )\n",
    "    elif i >= 21 :\n",
    "        loss_value = sess.run([train_op, loss],\n",
    "                          feed_dict=\n",
    "                          {TrainEEG_holder: Train_batch_f0l1[i], \n",
    "                           TrainLabel_holder:\n",
    "                           label_batch_1\n",
    "                          }\n",
    "                         )\n",
    "    '''loss_value = sess.run([train_op, loss],\n",
    "                          feed_dict=\n",
    "                          {TrainEEG_holder: image_batch, \n",
    "                           TrainLabel_holder:\n",
    "                           label_batch\n",
    "                          }\n",
    "                         )'''\n",
    "    #loss_value = sess.run([train_op, loss],\n",
    "\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    #if step % 10 == 0:\n",
    "    if step % 3 == 0:\n",
    "        examples_per_sec = batch_size / duration\n",
    "        sec_per_batch = float(duration)\n",
    "        \n",
    "        print('step is',step)\n",
    "        print('loss is',loss_value)\n",
    "        print('examples_per_sec is',examples_per_sec)\n",
    "        print('sec/batch：',sec_per_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision @ 1 = 0.625\n"
     ]
    }
   ],
   "source": [
    "#最后一个阶段，评测模型阶段，在测试集上测试\n",
    "num_examples = 800\n",
    "import math\n",
    "num_iter = int(math.ceil(num_examples/batch_size))\n",
    "true_count = 0\n",
    "total_sample_count = num_iter * batch_size\n",
    "step = 0\n",
    "j = -1\n",
    "while step < num_iter:\n",
    "    #image_batch,label_batch = sess.run([images_test,labels_test])\n",
    "    j = j+1\n",
    "    if j < 3:\n",
    "        predictions = sess.run([top_k_op],\n",
    "                         feed_dict=\n",
    "                         {TrainEEG_holder:Test_batch_f0l1[j],\n",
    "                          TrainLabel_holder:\n",
    "                          label_batch_0\n",
    "                         })\n",
    "        true_count += np.sum(predictions)\n",
    "        step +=1\n",
    "    elif j >= 3:\n",
    "        predictions = sess.run([top_k_op],\n",
    "                         feed_dict=\n",
    "                         {TrainEEG_holder:Test_batch_f0l1[j],\n",
    "                          TrainLabel_holder:\n",
    "                          label_batch_1\n",
    "                         })\n",
    "        true_count += np.sum(predictions)\n",
    "        step +=1\n",
    "    '''predition = sess.run([top_k_op],\n",
    "                         feed_dict=\n",
    "                         {image_holder:image_batch,\n",
    "                          label_holder:\n",
    "                          label_batch})\n",
    "    true_count += np.sum(predictions)\n",
    "    step +=1'''\n",
    "\n",
    "\n",
    "    \n",
    "precision = true_count/ total_sample_count\n",
    "print('precision @ 1 = %.3f' %precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
